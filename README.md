# DreamBooth Fine-Tuning for Stable Diffusion ğŸŒŸ

This repository demonstrates how to fine-tune Stable Diffusion using **DreamBooth** ğŸ‘Ÿ on a custom shoe dataset. It includes code, hyperparameter configurations, and notes on overcoming GPU/memory challenges ğŸ’ª.

---

## Project Overview ğŸ“‹

- **Goal** ğŸ¯: Teach Stable Diffusion to generate new images of a specific shoe, placed in various backgrounds and scenarios.  
- **Method** ğŸ› ï¸: DreamBooth fine-tuning on ~9 images of the same shoe from different angles ğŸ“¸.

---

## Changes Made

During the fine-tuning process, I adjusted key hyperparameters:

```bash
--train_batch_size=4
--learning_rate=3e-6
--max_train_steps=1400
```
## Benefits of These Changes

- **Larger Batch Size** ğŸ“ˆ: Enables more stable training on a high-VRAM GPU.
- **Lower Learning Rate** âš–ï¸: Facilitates gradual and precise updates, capturing intricate shoe details.
- **Extended Training Steps** â³: Promotes thorough learning while minimizing excessive overfitting.

## Challenges Encountered

### CUDA Version & Driver Issues ğŸ› ï¸
- Required ensuring the environment aligned with the GPUâ€™s driver and PyTorch specifications.

### Memory Limitations ğŸ’¾
- Despite using a large GPU, mid-training checkpoints and high-resolution images demanded careful disk space management.
- Occasionally required disabling intermediate checkpoints to mitigate memory constraints.

### Environment Mismatch ğŸ”§
- Needed to update the `diffusers` library to the correct version (`>= 0.33.0.dev0`) to prevent import errors.

## Hardware Specs ğŸ’»

- **GPU** ğŸ®: 48 GB VRAM (peak usage ~30 GB)
- **System RAM** ğŸ§ : 64 GB
- **Disk** ğŸ’¿: Sufficient space required for storing checkpoints; resolved â€œNo space left on deviceâ€ errors by clearing out old model folders.

## Future Improvements ğŸš€

- **Additional Images** ğŸ“¸: Collect more shoe angles to minimize stylization inconsistencies.
- **Further Hyperparameter Tuning** ğŸ›ï¸: Test different schedulers, warmup steps, or integrate LoRA for enhanced performance.
- **Mixed Precision** âš¡: Implement FP16 or BF16 to lower VRAM usage, potentially allowing for an increased batch size.
